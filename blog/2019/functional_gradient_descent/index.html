<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="google-site-verification=6CU-DoIp0FraaNmQsqgvxCw5swk1BHoouEqmFRoeWNc"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Functional Gradient Descent | Qiang HE </title> <meta name="author" content="Qiang HE"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sweetice.github.io/blog/2019/functional_gradient_descent/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Qiang HE </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Functional Gradient Descent</h1> <p class="post-meta"> December 25, 2019 </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL,</a>   <a href="/blog/tag/programming"> <i class="fa-solid fa-hashtag fa-sm"></i> Programming</a>     ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Claim: This is a backup of <a href="https://simple-complexities.github.io/optimization/functional/gradient/descent/2020/03/04/functional-gradient-descent.html" rel="external nofollow noopener" target="_blank">this post</a>!</p> <p>The content in this post has been adapted from <a href="http://www.cs.cmu.edu/~16831-f12/notes/F12/16831_lecture21_danielsm.pdf" rel="external nofollow noopener" target="_blank">Functional Gradient Descent - Part 1</a> and <a href="http://www.cs.cmu.edu/~16831-f14/notes/F10/16831_lecture24_varunnr/16831_lectureNov11.vramakri.pdf" rel="external nofollow noopener" target="_blank">Part 2</a>. Functional Gradient Descent was introduced in the NIPS publication <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf" rel="external nofollow noopener" target="_blank">Boosting Algorithms as Gradient Descent</a> by Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean in the year 2000.</p> <p>We are all familiar with gradient descent for linear functions \(f(x) = w^Tx\).<br> Once we define a loss \(L\), gradient descent does the following update steps (\(\eta\) is a parameter called the learning rate.): \[ w \rightarrow w - \eta \nabla L(w) \] where we move around in the space of weights. An example of a loss \(L\) is: \[ L(w) = \sum_{i=1}^n(y_i - w^Tx_i)^2 + \lambda\lVert w \rVert ^2 \] where the first term (the ‘L2’ term) measures how close \(f(x)\) is to \(y\), while the second term (the ‘regularization’ term) accounts for the ‘complexity’ of the learned function \(f\).</p> <p>Suppose we wanted to extend \(L\) to beyond linear functions \(f\). We want to minimize something like: \[ L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2 \] where \(\lVert f \rVert ^2\) again serves as a regularization term, and we have updates of the form: \[ f \rightarrow f - \eta \nabla L(f) \] where we move around in the space of functions, not weights!</p> <p>Turns out, this is completely possible! And goes by the name of ‘functional’ gradient descent, or gradient descent in function space.</p> <p>A question that one may have is: why do this in the first place? Every function can be parametrized, and we can do ‘ordinary’ gradient descent in the space of parameters, instead?</p> <p>The answer is: yes, you always can! In general, you can parametrize any function in a number of ways, each parametrization gives rise to different steps (and different functions at each step) in gradient descent.</p> <p>The advantage is that some loss functions that are non-convex when parametrized, can be convex in the function space: this means functional gradient descent can actually converge to global minima, when ‘ordinary’ gradient descent could possibly get stuck at local minima or saddle points.</p> <p>So, what does functional gradient descent mean?</p> <h3 id="functionals">Functionals</h3> <p>A functional is a function defined over functions, returning a real value.<br> Examples:</p> <ul> <li>The evaluation functional \[ E_x(f) = f(x) \]</li> <li>The sum functional \[ S_{{x_1, \ldots, x_n}}(f) = \sum_{i = 1}^n f(x_i) dx \]</li> <li>The integration functional \[ I_{[a, b]}(f) = \int_a^b f(x) dx \]</li> </ul> <p>It follows that the composition of a function \(g: \mathbb{R} \to \mathbb{R}\) with a functional is also a functional. The loss function \(L(f)\) defined above is a functional!</p> <h3 id="reproducing-kernel-hilbert-space">Reproducing Kernel Hilbert Space</h3> <p>It turns out that it is especially convenient when functions come from a special set, called a reproducing kernel Hilbert space. A Hilbert space can be thought of as a vector space, where we have the notion of an inner product between two elements. (This is not the complete definition, but this is what we’ll need).</p> <hr> <h4 id="review-kernels">Review: Kernels</h4> <p>A kernel \(K: X \times X \to \mathbb{R}\) is a function that generalizes dot products:</p> <ul> <li> <em>Symmetry</em>: For any \(x_i, x_j \in X\): \[ K(x_i, x_j) = K(x_j, x_i). \]</li> <li> <em>Positive Semi-Definiteness</em>: For any \(x_1, \ldots, x_n \in X\), the matrix \(K_M\) given by \[ K_{M_{ij}} = K(x_i, x_j) \] is positive semi-definite. Note that this implies \(K(x_i, x_j) \geq 0\) always.</li> </ul> <p>It turns out (Mercer’s condition) that these conditions are equivalent to \[ K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j) \] where \(\phi\) is a function that is sometimes called the ‘feature map’. Thus, a kernel can be thought of as the dot product in some feature space (the range of \(\phi\)). Similar to the dot product then, the kernel measures similarity between two inputs.</p> <p>Examples of kernel functions include:</p> <ul> <li>Linear Kernel: \[ K(x_i, x_j) = x_i \cdot x_j \]</li> <li>Polynomial Kernel (of degree \(d\)): \[ K(x_i, x_j) = (x_i \cdot x_j + c)^d \] The presence of the \(c\) term allows coefficients of degree less than \(d\) to be accommodated too.</li> <li>RBF Kernel (of ‘width’ \(\sigma\)): \[ K(x_i, x_j) = \exp\left(-\frac{\lVert x_i - x_j \rVert^2}{2\sigma^2}\right) \]</li> </ul> <p>Try to derive what the associated feature map is, for each of these kernels!</p> <hr> <p> </p> <p>We can now define a reproducing kernel Hilbert space or a ‘RKHS’.<br> A <em>reproducing kernel Hilbert space</em>, obtained on fixing a kernel \(K\), is a space of functions where every function \(f\) is some linear combination of the kernel \(K\) evaluated at some ‘centers’ \(x_{Cf}\): \[ f(x) = \sum_{i = 1}^n \alpha_{f_i} K(x, x_{Cf_i}) \] or, ignoring the argument \(x\): \[ f = \sum_{i = 1}^n \alpha_{f_i} K(\cdot, x_{Cf_i}) \] For a kernel \(K\) will denote the associated reproducing kernel Hilbert space by \(H_K\).<br> From the definition above, every \(f \in H_K\) is completely determined by the coefficients \(\alpha_f\) and the centers \(x_{Cf}\). Note that the number of centers (\(=\) dimension of \(\alpha_f\)) can vary between functions.</p> <p>We can now define the inner product (the ‘dot’ product) in \(H_K\) by: \[ f \cdot g = \sum_{i = 1}^{n_f} \sum_{j = 1}^{n_g} \alpha_{f_i} \alpha_{g_j} K(x_{Cf_i}, x_{Cg_j}) = \alpha_f K_{fg} \alpha_g \] where, \[ K_{{fg}_{ij}} = K(x_{Cf_i}, x_{Cg_j}). \] This inner product induces the norm \(\lVert \cdot \rVert\): \[ {\lVert f \rVert}^2 = f \cdot f = \alpha_f K_{ff} \alpha_f \geq 0. \] Why do we use the term reproducing? This is because we can ‘reproduce’ the value of \(f \in H_K\) at any \(x\) by taking the inner product of \(f\) with the ‘reproducing kernel’ function \(K(x, \cdot) \in H_K\): \[ f \cdot K(x, \cdot) = f(x). \] Verify this property!</p> <p>So, we’ve seen how to define the inner-product and norm \(\lVert f \rVert\) of any function \(f \in H_K\). But, in order to minimize via gradient descent, we need the definition of a derivative.</p> <h3 id="derivatives-of-functionals">Derivatives of Functionals</h3> <p>As reviewed in my previous post on <a href="/optimization/constrained/theory/2020/03/03/optimization-review.html">optimization theory</a>, one of the definitions of the derivative \(Df\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is: \[ \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{f(x + h) - (f(x) + Df(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0 \] where \(Df(x)\) is a size \(n\) row vector, with which the take the dot product of the direction \(h\) with.</p> <p>We may not be working in \(\mathbb{R}^n\) anymore, but this definition gives us that the derivative \(DE\) of a functional \(E\) on \(H_K\) must satisfy: \[ \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(x + h) - (E(x) + DE(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0 \] where \(h\) and \(x\) are now functions in \(H_K\), instead of points. This means that \(DE(x)\) is a function, too! (Recall that addition of functions occurs point-wise.)</p> <hr> <p><strong>Example 1:</strong> Let us take the example of the evaluation functional \(E_x(f) = f(x)\) and compute its derivative: \[ \begin{aligned} E(f + h) &amp;= (f + h)(x) \newline &amp;= f(x) + h(x) \newline &amp;= E(f) + h(x) \newline &amp;= E(f) + K(x, \cdot) \cdot h \newline \end{aligned} \] Thus, the derivative \(DE_x(f)\) is independent of \(f\), and is given by: \[ DE_x(f) = K(x, \cdot). \]</p> <p><strong>Example 2:</strong> Similarly, following the example of my reference material, the functional \(E(f) = {\lVert f \rVert}^2\) satisfies: \[ \begin{aligned} E(f + h) &amp;= {\lVert f + h \rVert}^2 \newline &amp;= {(f + h) \cdot (f + h)} \newline &amp;= {f \cdot f} + 2 {f \cdot h} + {h \cdot h} \newline &amp;= E(f) + 2 {f \cdot h} + {h \cdot h} \newline \end{aligned} \] Thus, the derivative \(DE(f)\) is defined as: \[ DE(f) = 2f. \] Note how similar this is to the derivative \(2x\) of the function \(x \to \lvert x \rvert^2\) on real numbers!</p> <hr> <p> </p> <h4 id="the-chain-rule">The Chain Rule</h4> <p>Very fortunately, we also have the chain rule! As discussed before, if \(E\) is a functional and \(g: \mathbb{R} \to \mathbb{R}\) is differentiable, then \(g(E)\) is also a functional, with derivative: \[ D(g(E))(f) = g’(E(f)) \ DE(f). \]</p> <hr> <p><strong>Example 3:</strong> Let us compute the derivative of the loss functional \(L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2\) with the chain rule: The individual terms in the first sum term is a composition of \[ g_i(x) = (y_i - x)^2 \text{ and } E_{x_i}. \] Thus, each of these terms has derivative: \[ \begin{aligned} D(g_i({x_i}))(f) &amp;= -2 (y_i - E_{x_i}(f)) \cdot DE_{x_i}(f) \newline &amp;= -2 (y_i - f(x_i)) \cdot K(x_i, \cdot). \end{aligned} \] The second term has derivative \(2\lambda f\), as derived above. Thus, the derivative \(DL(f)\) is given by: \[ DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f. \]</p> <hr> <p> </p> <p>There is one last point to note. When we take steps in ‘ordinary’ gradient descent, we move along the negative of the gradient vector because that is the direction along which the dot product with the gradient is minimum. No matter how this vector points! In some sense, we are not restricted to move in any direction. This is because our underlying domain is \(\mathbb{R}^n\).</p> <p>In ‘functional’ gradient descent, however, we are restricted to \(H_K\). How can we guarantee that when moving along \(DL(f)\), we do not stray out of \(H_K\)? One way to ensure that is by proving that we always have \(DL(f) \in H_K\). (This was true for our examples above! We have actually implicitly assumed this in our definition, too.) Then, closure of \(H_K\) under addition (and scalar multiplication) ensures that at every iteration, our current function \(f\) is in \(H_K\).</p> <p>This is what we will prove, in the next section.</p> <p>The <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf" rel="external nofollow noopener" target="_blank">Boosting Algorithms as Gradient Descent</a> paper above, does not use reproducing kernel Hilbert spaces, and actually applies to more general sets of functions. This is why they mention the fact that moving along the gradient is not always possible. Instead, they move along the direction with the least dot product with the gradient, among all directions that keeps them within their domain of functions. With reproducing kernel Hilbert spaces, this is not a problem: we are, fortunately, not restricted to move along the negative of the gradient.</p> <h3 id="h_k-is-closed-under-the-derivative">\(H_K\) is Closed under the Derivative</h3> <p>If \(E\) is a functional on \(H_K\), and \(f \in H_K\), then we always have: \[ DE(f) \in H_K. \]</p> <p>Let us define the derivative \(DE^*(f)\) as a functional: \[ \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(f + h) - (E(f) + DE^*(f)(h))}\rVert}{\lVert{h}\rVert} = 0 \] I differentiate between \(DE^*(f)\) (the functional) and \(DE(f)\) (the function). We want to show that, in fact: \[ DE^*(f)(h) = {\langle h, DE(f) \rangle}_K \]</p> <p>Note that \(DE^*(f)\) is a linear functional! Why? Using the definition above (and properties of the norm and limits):</p> <ul> <li>\(DE^*(f)(ch) = c \cdot DE^*(f)(h)\) where \(c \in \mathbb{R}\).</li> <li>\(DE^*(f)(h + g) = DE^*(f)(h) + DE^*(f)(g)\).</li> </ul> <p>This should not be surprising! We use \(DE^*(f)\) to give us the ‘best’ linear approximation around \(f\) along each direction \(h\).</p> <p>The <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem" rel="external nofollow noopener" target="_blank">Riesz Representation Theorem</a> then tells us that every linear functional \(L\) on a Hilbert space is actually of the form: \[ L = \langle \cdot, v \rangle \] for some \(v\) in the Hilbert space, where \(\langle \cdot, \cdot \rangle\) is the inner product in the Hilbert space.<br> For an RKHS, the inner product is given by the kernel \(K\), so, \[ DE^*(f) = {\langle \cdot, DE(f) \rangle}_K \] for some \(DE(f) \in H_K\). This means, we can write: \[ DE^*(f)(h) = {\langle h, DE(f) \rangle}_K \] where \(DE(f) \in H_K\), which is what we had to show!</p> <h3 id="an-example">An Example</h3> <p>Consider the regression problem, where \(x_i,\) for \(i \in {1, \ldots, 20}\) are linearly spaced in \([-1, 1]\): \[ y_i = e^{-\left(\frac{x_i - 0.5}{0.5}\right)^2} + e^{-\left(\frac{x_i + 0.5}{0.5}\right)^2} + \frac{\mathcal{N}(0, 1)}{20} \]</p> <p>Although this is a simple enough problem that would be easily solved by ‘ordinary’ gradient descent, we will demonstrate how functional gradient descent works here.</p> <p>First, we need a loss function. We will use the L2 loss with regularization, \(L(f)\), defined above. We have already seen in Example 3, that the gradient of \(L(f)\) is: \[ DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f. \] Let us define \(K\) as the RBF kernel with width \(0.5\). The presence of the Gaussian noise term above means the true hypothesis is not in \(H_K\), but we should get close!</p> <p>We initialize \(\alpha_{f_0}\) randomly, and set: \[ f_0 = \sum_{i = 1}^{20} \alpha_{f_0i} K(\cdot, x_i) \] and then start updating: \[ f_{t + 1} = f_t - \eta \cdot DL(f_t) \]</p> <p>This makes sense! But if we want to represent this in code, we would have to represent these functions in some way. One way is to maintain the coefficients \(\alpha_{f_t}\) and kernel centers \(x_{C{f_t}}\) at every iteration. We can simplify this by deciding to store only \(\alpha_{f_t}\) (allowing zeros) and implicitly use all \(x_i\) as the kernel centers. This is actually the same as doing gradient descent on \(\alpha_f\)! Other ways would be to add training samples one-by-one in an online manner, maintaining/recomputing the function values only at the training points. However, this causes a complication wherein the function is only defined at the training samples. To fix this, instead of updating by the gradient, we update using smooth functions that approximate the gradient: this is exactly gradient boosting!</p> <p>If we decide to represent our function implicitly by \(\alpha_f\) at each step, our updates are now: \[ \alpha_{f_{t + 1}} = 2 \eta (y - f_t(x)) + (1 - 2\lambda\eta) \alpha_{f_t} \] where \(y - f_t(x)\) is a vector with \((y - f_t(x))_i = y_i - f_t(x_i)\). Check this!</p> <p>If we implement all this, and plot the resulting learned hypothesis at each step of gradient descent:</p> <p style="text-align:center"><img src="/assets/images/functional_gradient_descent.gif" alt="Functional Gradient Descent Example" title="Functional Gradient Descent Example"></p> <p>indicating that functional gradient descent converges pretty fast, for our example. The code for this example is available <a href="https://github.com/simple-complexities/simple-complexities.github.io/tree/master/code/functional_gradient_descent.py" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="conclusion">Conclusion</h3> <p>We have seen:</p> <ul> <li>Why functional gradient descent can be useful,</li> <li>What it means to do functional gradient descent, and,</li> <li>How we can do functional gradient descent, with an example.</li> </ul> <p>and that’s all I have for today.</p> <p>I’m also starting to introduce a commenting facility via GitHub Issues, in order to not clutter up this space here. Comment <a href="https://github.com/simple-complexities/simple-complexities.github.io/issues/3" rel="external nofollow noopener" target="_blank">here</a>!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/docker/">An introduction to Docker for deep learning researcher</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/How_to_install_mpi4py_in_your_ubuntu/">How to installation mpi4py in your ubuntu</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/NVIDIA_SMI/">Fix NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/How_to_plot_figures_like_RL_papers/">How to plot performance figures in reinforcement learning papers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qiang HE. Last updated: June 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script>let theme=localStorage.getItem("theme");document.onreadystatechange=(()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-vega_lite").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let d=document.createElement("div");d.classList.add("vega-lite"),a.after(d),"dark"===theme?vegaEmbed(d,JSON.parse(t),{theme:"dark"}):vegaEmbed(d,JSON.parse(t))})});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-VRTHF8HTJD"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-VRTHF8HTJD");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>